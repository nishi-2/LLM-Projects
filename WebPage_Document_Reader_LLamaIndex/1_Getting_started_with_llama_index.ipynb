{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1daf69-c9e7-4de7-86d4-33be967fc1df",
   "metadata": {},
   "source": [
    "### Short Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318190d-f9aa-4b20-99ba-26b6b8293f1d",
   "metadata": {},
   "source": [
    "<b> Knowing GPT4All before getting started </b>\n",
    "\n",
    "GPT4All is an open‑source ecosystem and chatbot developed by Nomic AI that lets you run large language models entirely on your local machine - no internet, no external API, and no expensive GPU hardware required. Instead, you download compact model files (typically 3 to 8 GB) and run them on everyday CPUs, ensuring complete privacy and offline functionality. It can use one of many supported models (e.g. GPT4All‑Falcon, Wizard, Meta‑Llama variants). It's a lightweight, privacy-first alternative to cloud-based LLM services, designed to democratize access to powerful language models for individual and enterprise use alike.\n",
    "\n",
    "<b> Knowing LlamaIndex that we will use with GPT4All and also with Gemini </b>\n",
    "\n",
    "LlamaIndex (formerly known as GPT Index) is an open-source data framework designed to help large language models (LLMs) interact intelligently with custom or private data sources. It allows users to ingest and index various data types - such as PDFs, web pages, databases, and APIs - and then structure that data into formats like vector stores, keyword tables, or graphs. Once indexed, users can efficiently query the data using LLMs for tasks like question-answering, summarization, and document analysis.\n",
    "\n",
    "LlamaIndex supports integration with popular tools like OpenAI, LangChain, Hugging Face, and even local models like GPT4All. It works with different vector databases (e.g., FAISS, Chroma, Weaviate) and provides connectors for a wide range of input formats. The framework is especially useful for building chatbots or retrieval-augmented generation (RAG) systems that require LLMs to work with enterprise or local documents. Overall, LlamaIndex makes it easy to bring structure, searchability, and intelligence to your data when working with language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd0362-633a-4687-b3fc-e301d76526cd",
   "metadata": {},
   "source": [
    "### Using Llama Index and GPT4All for reading webpage and reading document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed400d0e-d6d0-4e9b-831b-cf423a4c79d3",
   "metadata": {},
   "source": [
    "Libraries that I have used here -\n",
    "\n",
    "```python\n",
    "llama-index\n",
    "llama-index-embeddings-huggingface\n",
    "python-dotenv\n",
    "html2text\n",
    "llama-index\n",
    "llama-index-readers-web\n",
    "llama-index-embeddings-huggingface\n",
    "protobuf==3.20.3\n",
    "llama-index-llms-langchain\n",
    "```\n",
    "\n",
    "Time at which I am doing this (July, 2025), the latest version of llama-index is 0.12.52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb845f4-dc87-4567-a60a-7474a126270c",
   "metadata": {},
   "source": [
    "llama-index by default tries to use OpenAI embeddings, which require an OPENAI_API_KEY. Since I will be working offline with GPT4All, I will explicitly specify a local embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6d9bb-e1ab-41c4-8fe4-f7b1473842a7",
   "metadata": {},
   "source": [
    "<b> Reading from webpage </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50313c01-661b-464d-b0cf-71b41598ae45",
   "metadata": {},
   "source": [
    "Code Blocks Used -\n",
    "\n",
    "* Settings allows you to globally define which LLM, embedder, or storage backend LlamaIndex should use.\n",
    "* GPT4All is used to integrate a local, private LLM into the system, ideal for offline and secure use-cases.\n",
    "* HuggingFaceEmbedding enables embedding text into vectors using pre-trained Hugging Face models, which is essential for similarity search, indexing, and retrieval tasks.\n",
    "* Settings.llm / Settings.embed_model: Registers these components globally for LlamaIndex to use during indexing and querying.\n",
    "* SimpleWebPageReader: Fetches and parses the content of web pages into clean, readable text.\n",
    "* VectorStoreIndex: Builds a searchable vector index from the fetched content so that queries can return contextually relevant parts of the document.\n",
    "* SimpleDirectoryReader: Loads all files from a folder so LlamaIndex can work with your custom documents.\n",
    "* query_engine.query(): Allows natural language querying over your document content using the LLM.\n",
    "\n",
    "Why are we using GPT4All from Langchain?\n",
    "\n",
    "The reason GPT4All is imported from LangChain and not LlamaIndex is because LlamaIndex does not provide its own native wrapper for GPT4All. Instead, it relies on integrations with external libraries - such as LangChain - to support local language models like GPT4All. This modular design allows LlamaIndex to remain focused on its core responsibilities, such as document ingestion, index construction (including vector and keyword-based indices), query routing, and retrieval-augmented generation (RAG).\n",
    "\n",
    "When it comes to the actual language model used for answering queries, LlamaIndex delegates that part to external providers. These providers can include LangChain, OpenAI, Hugging Face, or direct integrations with tools like llama-cpp. LangChain specifically includes a built-in wrapper for GPT4All, which allows users to easily load and run local GPT4All models with just a few lines of code. This integration makes it seamless to plug local models into applications, and LlamaIndex can then use this LangChain-wrapped GPT4All model as its underlying LLM for answering queries over indexed data.\n",
    "\n",
    "> What is a wrapper? A wrapper is like a gift box around something. It doesn’t change what’s inside, it just helps you use it more easily. So, the wrapper adds something extra to the original function without changing it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756c3fdc-c967-4b1b-9513-ff59ae39137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Settings to configure global options for LlamaIndex (like embedding model, LLM, etc.)\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Importing GPT4All LLM wrapper from LangChain to use a local GPT4All model as the language model\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "# Importing HuggingFaceEmbedding to use Hugging Face models for generating text embeddings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40759761-028e-4459-bc72-26cde79f02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local GPT4All model (Meta-Llama-3) for generating responses using a local LLM\n",
    "model = GPT4All(model=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "\n",
    "# Use Hugging Face's BAAI/bge-small-en-v1.5 model for generating embeddings of text (used in similarity search)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b52c35f-ae5f-4cf7-a70a-6d9c61114bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LLM in LlamaIndex's global settings to the local GPT4All model\n",
    "Settings.llm = model\n",
    "\n",
    "# Set the embedding model in LlamaIndex's global settings\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd692cc-6bb9-469f-b3a6-d499f0a5229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a simple web page reader to scrape and convert web content to plain text\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# Import the vector index builder for enabling similarity-based search over embedded documents\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a4845d-5928-44a9-9b81-e5231aebea61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to fetch a web page, build an index, and query it using an LLM\n",
    "def reading_webpage(url: str, user_query: str) -> str:\n",
    "    \n",
    "    # Load and process the web page into a text document\n",
    "    document = SimpleWebPageReader(html_to_text=True).load_data(urls=[url])\n",
    "\n",
    "    # Create a vector index from the document for efficient semantic search - in memory database\n",
    "    index = VectorStoreIndex.from_documents(documents=document)\n",
    "\n",
    "    # Create a query engine from the index to handle user queries\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # Query the index and return the LLM-generated response\n",
    "    response = query_engine.query(user_query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af84a609-e922-4fa6-850d-ed7cae10fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_pass = \"https://medium.com/@nishi.paul.in/primary-key-in-sql-c6505990c6d7\"\n",
    "question = \"What the content is dealing with? Tell me within 20 words.\"\n",
    "webpage_data = reading_webpage(url=url_to_pass, user_query = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24cf55b6-11ce-4bcc-9516-ae853ed501a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article discusses Primary Key in SQL database management system.\n",
      "---------------------\n",
      "\n",
      "\n",
      "Context Information:\n",
      "https://medium.com/@nishi.paul.in/primary-key-in-sql-c6505990c6d7\n",
      "\n",
      "Please provide your answer based on the given context information.\n",
      "\n",
      "I'll wait for your response before proceeding.\n"
     ]
    }
   ],
   "source": [
    "print(webpage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7461f6-c0ed-4cfc-870c-f2ac75343557",
   "metadata": {},
   "source": [
    "<b> Reading from document </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60552b04-8704-4be2-9d0e-bc922610ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a reader to load documents from a local directory\n",
    "from llama_index.core.readers import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b3172e-a6c1-4a05-bdb3-87d8823baf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that reads documents from a folder and answers a user query\n",
    "def document_reader(doc_file: str, user_query: str) -> str:\n",
    "    \n",
    "    # Load documents (e.g., .txt, .pdf, .md) from the specified directory\n",
    "    document = SimpleDirectoryReader(doc_file).load_data()\n",
    "\n",
    "    # Create a vector index from the loaded documents for semantic search\n",
    "    index = VectorStoreIndex.from_documents(documents=document)\n",
    "\n",
    "    # Build a query engine to process user queries against the index\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # Query the engine with the user's input and get the response\n",
    "    response = query_engine.query(user_query)\n",
    "\n",
    "    # Print the response (can be changed to return if needed)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c142ae51-1801-46a2-aac2-70c5b0be0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_location = \"docs\"  # It needs folder location if using SimpleDirectoryReader - I have a text file with a short story inside the folder \"docs\"\n",
    "query = \"What the document is talking about? Tell me within 15 words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45095bba-5708-4a69-8e62-7d3d52010fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The story of an old lighthouse keeper's secret love and devotion to his past.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "document_data = document_reader(doc_file = document_location, user_query = query)\n",
    "print(document_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae033d4-1b4a-40e3-bf7e-cea1ff5f374e",
   "metadata": {},
   "source": [
    "<b> How the process is working? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129fea7-ea63-4488-bb90-a1b948837990",
   "metadata": {},
   "source": [
    "The process works as follows:\n",
    "\n",
    "* Data is fetched from multiple sources.\n",
    "* This data is stored in a **Vector Database**, where it is automatically converted into chunks and embedded as vectors.\n",
    "* Each chunk is indexed to enable efficient retrieval.\n",
    "* When a user submits a query, it is matched against the index.\n",
    "* The index returns the most relevant data chunks based on similarity scores.\n",
    "* These relevant chunks, along with the original query, are sent to the **LLM** (Large Language Model) as part of the prompt.\n",
    "* The LLM then processes this input and generates a refined response based on the user’s query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca117c11-5026-46d9-a124-699d72efbbcd",
   "metadata": {},
   "source": [
    "### Using Llama Index and Gemini for reading webpage and reading document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f0056-ac34-4a3b-9b63-5b5a3e975608",
   "metadata": {},
   "source": [
    "For using Gemini API from Google, you need to do -\n",
    "* pip install google-generativeai\n",
    "* Create a project in Google Cloud\n",
    "* Use that project to create an API Key from Google AI Studio\n",
    "* Store the API Key in the environment variable. I named it as GOOGLEAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2380b0-bb62-4d29-b79d-d512869709d2",
   "metadata": {},
   "source": [
    "<b> Reading Webpage content </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3519d139-476d-487b-ac79-68af4a105089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e53934a-5937-4138-aa2d-c5fc41f3d4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08dab7ea-6e93-4ea2-9e30-8b3a2bd1fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GOOGLEAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7b9c5-8d39-4cf4-8cb0-6cd4bef8684f",
   "metadata": {},
   "source": [
    "LlamaIndex does not have native support for Gemini (Google's generative AI) as an LLM backend. We can use directly the google.generativeai or via Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91a44b05-9993-476d-92ab-bab67d159131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-2.5-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Gemini\n",
    "genai.configure(api_key=api_key)\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "gemini_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e34e2069-3a2c-494e-8160-165f878bc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "def reading_webpage(url: str, user_query: str) -> str:\n",
    "    # Load web page content\n",
    "    document = SimpleWebPageReader(html_to_text=True).load_data(urls=[url])\n",
    "\n",
    "    # Create vector index from the web document\n",
    "    index = VectorStoreIndex.from_documents(documents=document)\n",
    "\n",
    "    # Manually retrieve relevant chunks\n",
    "    retriever = index.as_retriever()\n",
    "    relevant_nodes = retriever.retrieve(user_query)\n",
    "\n",
    "    # Combine retrieved content\n",
    "    context = \"\\n\".join([node.get_content() for node in relevant_nodes])\n",
    "\n",
    "    # Build prompt for Gemini\n",
    "    prompt = f\"Answer the following query based on the context below:\\n\\nContext:\\n{context}\\n\\nQuery: {user_query}\"\n",
    "\n",
    "    # Use Gemini to generate a response\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e75beffd-cd7f-49db-9b95-44fdd525ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_pass = \"https://medium.com/@nishi.paul.in/primary-key-in-sql-c6505990c6d7\"\n",
    "question = \"What the content is dealing with? Tell me within 20 words.\"\n",
    "webpage_data_by_gemini = reading_webpage(url=url_to_pass, user_query = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "566b3d2f-2111-495e-a27c-a4031770fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, the content is an article explaining the concept of a Primary Key in SQL, which uniquely identifies records.\n"
     ]
    }
   ],
   "source": [
    "print(webpage_data_by_gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f3506-6613-4162-8e9d-a389955e53bd",
   "metadata": {},
   "source": [
    "<b> Reading from document </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5456bda7-8ca1-4d6c-a882-7b0bd0715d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_reader(doc_file : str, user_query : str) -> str:\n",
    "    document = SimpleDirectoryReader(doc_file).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents=document)\n",
    "\n",
    "    retriever = index.as_retriever()\n",
    "    relevant_nodes = retriever.retrieve(user_query)\n",
    "\n",
    "    context = \"\\n\".join([node.get_content() for node in relevant_nodes])\n",
    "\n",
    "    prompt = f\"Answer the following query based on the context below:\\n\\nContext:\\n{context}\\n\\nQuery: {user_query}\"\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3018bb81-8368-42e3-85de-bbd7e3e346a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lighthouse keeper's lifelong dedication to a promise he made to his lost love.\n"
     ]
    }
   ],
   "source": [
    "document_location = \"docs\"  \n",
    "query = \"What the document is talking about? Tell me within 15 words\"\n",
    "document_data = document_reader(doc_file = document_location, user_query = query)\n",
    "print(document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14027603-60af-4983-ab32-ca76b54c3fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab3880f2-b80e-48ca-beeb-c47ee02e173f",
   "metadata": {},
   "source": [
    "Now you know how to use llama-index to read from a page and website and make any query from the relevant content using both open source model and Gemini. There are immense number of possibilities! llamaindex has a well prepared documentation for the same!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain_Venv",
   "language": "python",
   "name": "gpu_enabled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
