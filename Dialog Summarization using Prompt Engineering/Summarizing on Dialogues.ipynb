{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19ef888",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d608376",
   "metadata": {},
   "source": [
    "This project focuses on **dialogue summarization**, which involves generating concise and coherent summaries of multi-turn conversations. By leveraging pre-trained language models like **FLAN-T5**, we aim to capture the essential points, intentions, and context of dialogues, ensuring the summary is both informative and contextually accurate. The project explores techniques such as **zero-shot**, **one-shot** and **few-shot** prompting to guide the model in summarizing conversations without fine-tuning, using carefully crafted prompts. We also evaluate different configurations of the model's generation parameters to optimize the quality and relevance of the summaries, addressing challenges like handling long conversations and nuanced context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ca7ef",
   "metadata": {},
   "source": [
    "### Library installations and importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713b4da",
   "metadata": {},
   "source": [
    "We will install the required libraries at first. The specified versions might give some warning on dependencies but these are good to go with - <br/>\n",
    "pip install torch==1.13.1 <br/>\n",
    "pip install torchdata==0.5.1 <br/>\n",
    "pip install datasets==2.17.0 <br/>\n",
    "pip install transformers==4.27.2 <br/>\n",
    "You might also need to install pip install py7zr <br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The next step is to import the necessary libraries. First we will import the load_dataset library from the datasets module. load_dataset allows us to load datasets from the Hugging Face Hub or local files in various formats like JSON, CSV, etc. This is necessary to work with pre-defined datasets like samsum or custom datasets for dialogue summarization. Then we will import AutoModelForSeq2SeqLM from transformers, a generic class for models that perform sequence-to-sequence (seq2seq) tasks, such as summarization, translation, or question answering. It simplifies loading pre-trained seq2seq models from the Hugging Face Hub. Next we will import AutoTokenizer from transformers, which provides the tokenizer corresponding to the pre-trained model. The tokenizer is essential to convert raw text into token IDs the model can process. At last we will import GenerationConfig from transformers, a utility to configure parameters (e.g., max tokens, beam search) for text generation tasks. This allows customization of the text generation process when generating summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4713be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2e44f",
   "metadata": {},
   "source": [
    "### Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb5a01",
   "metadata": {},
   "source": [
    "First, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. <br/> \n",
    "\n",
    "We will use two types of datasets - <br/>\n",
    "\n",
    "The samsum dataset consists of dialogues and corresponding human-written summaries. It is commonly used for testing and fine-tuning dialogue summarization models. <br/>\n",
    "\n",
    "The knkarthick/dialogsum dataset on Hugging Face is designed for dialogue summarization tasks. It contains a collection of dialogue texts sourced from various domains, such as daily conversations, interviews, and chats, along with corresponding human-written summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d2ce1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|███████████████████████████████████████████| 14732/14732 [00:01<00:00, 9098.76 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████| 819/819 [00:00<00:00, 5249.17 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████| 818/818 [00:00<00:00, 4325.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Specify the name of the dataset to be used\n",
    "huggingface_dataset_name_dialogsum =  \"knkarthick/dialogsum\"  \n",
    "huggingface_dataset_name_samsum = \"samsum\"\n",
    "\n",
    "\n",
    "# Loads the specified dataset from the Hugging Face Hub into memory.\n",
    "dataset_dialogsum = load_dataset(huggingface_dataset_name_dialogsum)  \n",
    "dataset_samsum = load_dataset(huggingface_dataset_name_samsum, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddbcf6",
   "metadata": {},
   "source": [
    "Dataset is a dictionary-like object with splits like train, validation, and test. Each split contains records or data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e93e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dialogsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cdd73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a89086",
   "metadata": {},
   "source": [
    " Printing a couple of dialogues with baseline summaries -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddd6fb12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Number :   1\n",
      "\n",
      "INPUT DIALOGUE:    \n",
      "Gabby: How is you? Settling into the new house OK?\r\n",
      "Sandra: Good. The kids and the rest of the menagerie are doing fine. The dogs absolutely love the new garden. Plenty of room to dig and run around.\r\n",
      "Gabby: What about the hubby?\r\n",
      "Sandra: Well, apart from being his usual grumpy self I guess he's doing OK.\r\n",
      "Gabby: :-D yeah sounds about right for Jim.\r\n",
      "Sandra: He's a man of few words. No surprises there. Give him a backyard shed and that's the last you'll see of him for months.\r\n",
      "Gabby: LOL that describes most men I know.\r\n",
      "Sandra: Ain't that the truth! \r\n",
      "Gabby: Sure is. :-) My one might as well move into the garage. Always tinkering and building something in there.\r\n",
      "Sandra: Ever wondered what he's doing in there?\r\n",
      "Gabby: All the time. But he keeps the place locked.\r\n",
      "Sandra: Prolly building a portable teleporter or something. ;-)\r\n",
      "Gabby: Or a time machine... LOL\r\n",
      "Sandra: Or a new greatly improved Rabbit :-P\r\n",
      "Gabby: I wish... Lmfao!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "SUMMARY:   \n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Example Number :   2\n",
      "\n",
      "INPUT DIALOGUE:    \n",
      "Rachel: HAPPY NEW YEAAAAAAR!! ヽ(^o^)丿\r\n",
      "Rachel: from me & Tom :)\r\n",
      "Jack: Thanks!!\r\n",
      "Miranda: same to you\r\n",
      "Miranda: :)\r\n",
      "Rachel: May you get exactly what you are expecting in the coming year!!!\r\n",
      "Miranda: :*\r\n",
      "Jack: How's your party?\r\n",
      "Rachel: %) %) %) \r\n",
      "Jack: Hahahah\r\n",
      "Jack: I see\r\n",
      "Rachel: the best party eveeeeeeeer\r\n",
      "Rachel: wish you were here!!!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "SUMMARY:   \n",
      "Rachel wishes a happy new year from her and Tom to Miranda and Jack. Rachel is having the best time but misses Miranda and Jack.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Example Number :   3\n",
      "\n",
      "INPUT DIALOGUE:    \n",
      "Freddie: Just finished House of Cards, any recommendations?\r\n",
      "John: Versailles! I know it's different, but if you're into politics and scheming that's your show.\r\n",
      "Olivia: I agree, I watched it all, highly recommend.\r\n",
      "Freddie: Is it on Netflix? Can't find it.\r\n",
      "Olivia: No, only Canal+ :(\r\n",
      "May: How to Get Away With Murder is also great, it's also on Netflix. I'm currently waiting for the new season as they upload with a year long delay.\r\n",
      "Freddie: How many seasons are there now?\r\n",
      "May: Five, six coming in 2019.\r\n",
      "John: Might look into it as well. Have you seen Black Mirror?\r\n",
      "Freddie: Heard about it, but haven't started yet. Is it any good?\r\n",
      "Olivia: Hard to tell, couldn't finish the first episode with the pig :x\r\n",
      "Freddie: Pig? :D\r\n",
      "May: Well, yeah, there's that one really twisted episode, the first one. You can always skip it as every episode tells a different story. All of them are pretty twisted, but the first one was the worst for me as well.\r\n",
      "John: It's hard to tell guys if you are encouraging or discouraging him to watch it :D\r\n",
      "May: Just being honest, John ;) But I agree HTGAWM is a safer option, however weird it may sound. \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "SUMMARY:   \n",
      "John and May recommended Freddie to watch Versailles and How to Get Away With Murder.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [100, 200, 300]  # Looking at the records at indexes 100, 200, and 300 for both the datasets\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print('Example Number :  ', i + 1)\n",
    "    print()\n",
    "    print('INPUT DIALOGUE:    ')\n",
    "    print(dataset_samsum['train'][index]['dialogue'])\n",
    "    print('-'*120)\n",
    "    print('SUMMARY:   ')\n",
    "    print(dataset_samsum['train'][index]['summary'])\n",
    "    print('-'*120)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf9e9d58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Number :   1\n",
      "\n",
      "TOPIC:     cable\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:    \n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "SUMMARY:   \n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Example Number :   2\n",
      "\n",
      "TOPIC:     ask questions\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:    \n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "SUMMARY:   \n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Example Number :   3\n",
      "\n",
      "TOPIC:     cleaning unit introduction\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:    \n",
      "#Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "SUMMARY:   \n",
      "#Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [100, 200, 300]  # Looking at the records at indexes 100, 200, and 300 for both the datasets\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print('Example Number :  ', i + 1)\n",
    "    print()\n",
    "    print('TOPIC:    ', dataset_dialogsum['train'][index]['topic'])\n",
    "    print('-'*120)\n",
    "    print('INPUT DIALOGUE:    ')\n",
    "    print(dataset_dialogsum['train'][index]['dialogue'])\n",
    "    print('-'*120)\n",
    "    print('SUMMARY:   ')\n",
    "    print(dataset_dialogsum['train'][index]['summary'])\n",
    "    print('-'*120)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1675704",
   "metadata": {},
   "source": [
    "Load the FLAN-T5 model, creating an instance of the AutoModelForSeq2SeqLM class with the .from_pretrained() method. Flan-t5-large is a larger model with 780M parameters, and is publicly available on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d641467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'  # Specifies the name of the pre-trained model to be used.\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # Loads the specified pre-trained seq2seq model into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b00c8",
   "metadata": {},
   "source": [
    "To enable encoding and decoding of text with a language model, it is essential to work with text in a **tokenized form**. Tokenization is the process of breaking down a string of text into smaller, manageable units called **tokens**. These tokens can represent words, subwords, or even individual characters, depending on the tokenizer's configuration. This step is critical because Large Language Models (LLMs) like FLAN-T5 are designed to process numerical representations of tokens rather than raw text, ensuring efficiency and consistency in handling diverse inputs.\n",
    "\n",
    "To tokenize text for the FLAN-T5 model, we will download and initialize the tokenizer using the `AutoTokenizer.from_pretrained()` method from the Transformers library. This method retrieves the pre-trained tokenizer associated with the FLAN-T5 model, ensuring compatibility with its architecture. Additionally, you can utilize the `use_fast` parameter, which activates the **fast tokenizer** implementation. The fast tokenizer leverages the **Hugging Face Tokenizers library**, offering improved speed and lower memory usage without compromising accuracy—ideal for handling large-scale text processing tasks.\n",
    "\n",
    "For more information, you can refer to the documentation - https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4a57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  # Loads the tokenizer corresponding to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29b9da",
   "metadata": {},
   "source": [
    "Let's test the tokenizer encoding and decoding with a simple sentence. In the tokenizer object, we mentioned return_tensors='pt'. This specifies that the tokenized data should be returned as a PyTorch tensor (pt stands for PyTorch).\n",
    "This format is essential when using the tokenized data with PyTorch-based models. sentence_encoded is now a dictionary containing - input_ids: Numerical token IDs representing the input text.\n",
    "attention_mask: Indicates which tokens are meaningful (1) and which are padding (0). Now skip_special_tokens in the decode function of the tokenizer ensures that special tokens (e.g., [CLS], [SEP], [PAD]) added during tokenization are not included in the decoded text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "298fb202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:    tensor([   27,   333,  1036,    81,     3,   195,    51,     5,   301, 11160,\n",
      "           19,   694,    55,     1])\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "DECODED SENTENCE:    I love learning about llm. LLM is fun!\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love learning about llm. LLM is fun!\"\n",
    "\n",
    "# Converts the raw input text (sentence) into tokenized form using the tokenizer.\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "\n",
    "# Begins the process of converting tokenized IDs back into a human-readable string.\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0],  # Selects the token IDs for the first input as there may be a batch of inputs.\n",
    "        skip_special_tokens=True  \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "print('ENCODED SENTENCE:   ', sentence_encoded[\"input_ids\"][0])\n",
    "print('-'*120)\n",
    "print('DECODED SENTENCE:   ', sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bace70",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task. Now below we have used model.generate(), what is that? This line generates text (e.g., a summary, response, or continuation) using a pre-trained model based on the tokenized input provided. The generate method is used to create new sequences (e.g., tokens) based on the input provided.\n",
    "The model uses its underlying architecture and weights (in this case FLAN-T5) to predict the next tokens iteratively, forming coherent text output. max_new_token parameter specifies the maximum number of new tokens that the model can generate during this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f944d128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Gabby: How is you? Settling into the new house OK?\n",
      "Sandra: Good. The kids and the rest of the menagerie are doing fine. The dogs absolutely love the new garden. Plenty of room to dig and run around.\n",
      "Gabby: What about the hubby?\n",
      "Sandra: Well, apart from being his usual grumpy self I guess he's doing OK.\n",
      "Gabby: :-D yeah sounds about right for Jim.\n",
      "Sandra: He's a man of few words. No surprises there. Give him a backyard shed and that's the last you'll see of him for months.\n",
      "Gabby: LOL that describes most men I know.\n",
      "Sandra: Ain't that the truth! \n",
      "Gabby: Sure is. :-) My one might as well move into the garage. Always tinkering and building something in there.\n",
      "Sandra: Ever wondered what he's doing in there?\n",
      "Gabby: All the time. But he keeps the place locked.\n",
      "Sandra: Prolly building a portable teleporter or something. ;-)\n",
      "Gabby: Or a time machine... LOL\n",
      "Sandra: Or a new greatly improved Rabbit :-P\n",
      "Gabby: I wish... Lmfao!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Sandra and Jim are moving into the new house. Sandra's husband Jim is grumpy and keeps the garage locked. Sandra's husband is building a portable teleporter or a time machine.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Rachel: HAPPY NEW YEAAAAAAR!! ヽ(^o^)丿\n",
      "Rachel: from me & Tom :)\n",
      "Jack: Thanks!!\n",
      "Miranda: same to you\n",
      "Miranda: :)\n",
      "Rachel: May you get exactly what you are expecting in the coming year!!!\n",
      "Miranda: :*\n",
      "Jack: How's your party?\n",
      "Rachel: %) %) %) \n",
      "Jack: Hahahah\n",
      "Jack: I see\n",
      "Rachel: the best party eveeeeeeeer\n",
      "Rachel: wish you were here!!!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "Rachel wishes a happy new year from her and Tom to Miranda and Jack. Rachel is having the best time but misses Miranda and Jack.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "Rachel and Tom have a party.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Freddie: Just finished House of Cards, any recommendations?\n",
      "John: Versailles! I know it's different, but if you're into politics and scheming that's your show.\n",
      "Olivia: I agree, I watched it all, highly recommend.\n",
      "Freddie: Is it on Netflix? Can't find it.\n",
      "Olivia: No, only Canal+ :(\n",
      "May: How to Get Away With Murder is also great, it's also on Netflix. I'm currently waiting for the new season as they upload with a year long delay.\n",
      "Freddie: How many seasons are there now?\n",
      "May: Five, six coming in 2019.\n",
      "John: Might look into it as well. Have you seen Black Mirror?\n",
      "Freddie: Heard about it, but haven't started yet. Is it any good?\n",
      "Olivia: Hard to tell, couldn't finish the first episode with the pig :x\n",
      "Freddie: Pig? :D\n",
      "May: Well, yeah, there's that one really twisted episode, the first one. You can always skip it as every episode tells a different story. All of them are pretty twisted, but the first one was the worst for me as well.\n",
      "John: It's hard to tell guys if you are encouraging or discouraging him to watch it :D\n",
      "May: Just being honest, John ;) But I agree HTGAWM is a safer option, however weird it may sound. \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "John and May recommended Freddie to watch Versailles and How to Get Away With Murder.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "John recommends Versailles and How to Get Away With Murder. Olivia recommends How to Get Away With Murder and Black Mirror.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset_samsum['train'][index]['dialogue']\n",
    "    summary = dataset_samsum['train'][index]['summary']\n",
    "    \n",
    "    # Tokenization\n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    \n",
    "    # Model prediction with tokenized data\n",
    "    model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    "    # Detokenization\n",
    "    outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "    \n",
    "    print('-'*120)\n",
    "    print('Example Number:   ', i + 1)\n",
    "    print('-'*120)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print('-'*120)\n",
    "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
    "    print('-'*120)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{outputs}\\n')\n",
    "    print('-'*120)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7498a928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: My cable is down right now. #Person2#: The cable is down right now. #Person1#: I'm sorry. #Person2#: I'm sorry. #Person1#: I'm sorry. #Person2#: I'm sorry. #Person1#: I'm sorry. #Person2#: I'm sorry. #Person1#: I'\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: How are you doing at college? #Person1#: I am doing well in math. #Person2#: How are your schooling? #Person1#: I am doing well in science. #Person2#: How are your friends? #Person1#: I am a good friend of mine. #Person2#: How are you doing at work? #Person1#: I am doing well\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: Yes, please.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try with the other dataset\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset_dialogsum['train'][index]['dialogue']\n",
    "    summary = dataset_dialogsum['train'][index]['summary']\n",
    "    \n",
    "    # Tokenization\n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    \n",
    "    # Model prediction with tokenized data\n",
    "    model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    "    # Detokenization\n",
    "    outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "    \n",
    "    print('-'*120)\n",
    "    print('Example Number:   ', i + 1)\n",
    "    print('-'*120)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print('-'*120)\n",
    "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
    "    print('-'*120)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{outputs}\\n')\n",
    "    print('-'*120)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b145b34",
   "metadata": {},
   "source": [
    "You may notice that the model's predictions appear somewhat reasonable, as they align with the context of the input. However, the model does not seem entirely certain about the specific task it is expected to perform. Instead, it tends to generate a continuation of the dialogue that feels improvised or arbitrary rather than purposeful. This behavior suggests that the model lacks clarity about the intended outcome, such as summarizing, translating, or answering a question. In such cases, **prompt engineering**—the process of crafting clear, specific, and informative input prompts—can significantly improve the model's performance by guiding it more effectively toward the desired goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac06be",
   "metadata": {},
   "source": [
    "### Summarize Dialogue with an Instruction Prompt <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24f8a8",
   "metadata": {},
   "source": [
    "Prompt engineering is the practice of designing effective input prompts to guide language models (LLMs) like GPT or T5 to produce desired outputs. It leverages the model's inherent training on diverse data, enabling task-specific results without modifying the model's architecture. <br/>\n",
    "\n",
    "Ways of Prompt Engineering:\n",
    "Instruction-based Prompts: Clearly define the task, e.g., \"Summarize this passage.\"\n",
    "Few-shot Prompts: Provide examples within the prompt to guide the model.\n",
    "Zero-shot Prompts: Assume the model understands the task solely from the instruction.\n",
    "Chain-of-thought Prompts: Encourage step-by-step reasoning for complex tasks.\n",
    "Role-based Prompts: Assign roles like, “Act as a teacher explaining...” <br/>\n",
    "\n",
    "Usefulness:\n",
    "Prompt engineering is effective for:\n",
    "\n",
    "Text generation tasks (summarization, question answering, translation).\n",
    "Tasks where models must follow nuanced instructions.\n",
    "Prototyping without costly fine-tuning.\n",
    "<br/>\n",
    "\n",
    "Limitations:\n",
    "Reliability: Output varies with subtle prompt changes.\n",
    "Complex Tasks: Struggles with intricate domain-specific applications.\n",
    "Model Dependency: Performance hinges on the pre-trained model's limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a77c4",
   "metadata": {},
   "source": [
    "#### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef190c5",
   "metadata": {},
   "source": [
    "Zero-shot inference with an instruction prompt involves guiding a pre-trained language model (LLM) to perform a task it hasn't been explicitly trained on, without providing task-specific examples. Instead, a well-crafted instruction prompt is given to help the model understand the task from its general training knowledge. <br/>\n",
    "\n",
    "The LLM, like GPT-3 or FLAN-T5, relies on its extensive training on diverse text datasets, enabling it to infer tasks from natural language instructions. For instance, a prompt such as: \"Summarize the following text: [text]\" tells the model to generate a concise summary, even if summarization isn’t its primary task. <br/>\n",
    "\n",
    "Benefits:\n",
    "Flexibility: Adapts to multiple tasks without retraining.\n",
    "Cost-Efficiency: Avoids the need for labeled data or fine-tuning.\n",
    "Fast Prototyping: Quickly evaluates model performance on new tasks.\n",
    "<br/>\n",
    "\n",
    "Key Techniques:\n",
    "Direct Instruction: Straightforward commands like “Translate this text into French.”\n",
    "Role Assignment: Specifying a role, e.g., “You are a helpful assistant. Write a formal email.”\n",
    "Explicit Output Structure: Defining the desired format, e.g., “Answer in bullet points.”<br/>\n",
    "\n",
    "Challenges:\n",
    "Ambiguity: Vague instructions yield inconsistent outputs.\n",
    "Complexity: Struggles with specialized domains requiring expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aecd89f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I have a cable out. #Person2#: The cable is down right now. #Person1#: I'm sorry. #Person2#: I'm sorry. #Person1#: I'm sorry. #Person2#: I'm sorry. #Person1#: I'm sorry. #Person2#: I'm sorry. #Person1#: I'\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "                                                 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'd like to see our new cleaning unit.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset_dialogsum['train'][index]['dialogue']\n",
    "    summary = dataset_dialogsum['train'][index]['summary']\n",
    "\n",
    "    \n",
    "    # Here in this prompt we are not providing any example\n",
    "    # Rather we are just instructing to summarize the input which is a conversation in our case\n",
    "    prompt = f\"\"\"\n",
    "                Summarize the following conversation.\n",
    "                {dialogue}\n",
    "                Summary:\n",
    "            \"\"\"\n",
    "    \n",
    "\n",
    "    # We pass prompt in place of dialogue in the tokenizer.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Model prediction with tokenized data\n",
    "    model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    "    # Detokenization\n",
    "    outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    print('-'*120)\n",
    "    print('Example Number:   ', i + 1)\n",
    "    print('-'*120)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print('-'*120)\n",
    "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
    "    print('-'*120)  \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{outputs}\\n')\n",
    "    print('-'*120)  \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d17f5352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Gabby: How is you? Settling into the new house OK?\n",
      "Sandra: Good. The kids and the rest of the menagerie are doing fine. The dogs absolutely love the new garden. Plenty of room to dig and run around.\n",
      "Gabby: What about the hubby?\n",
      "Sandra: Well, apart from being his usual grumpy self I guess he's doing OK.\n",
      "Gabby: :-D yeah sounds about right for Jim.\n",
      "Sandra: He's a man of few words. No surprises there. Give him a backyard shed and that's the last you'll see of him for months.\n",
      "Gabby: LOL that describes most men I know.\n",
      "Sandra: Ain't that the truth! \n",
      "Gabby: Sure is. :-) My one might as well move into the garage. Always tinkering and building something in there.\n",
      "Sandra: Ever wondered what he's doing in there?\n",
      "Gabby: All the time. But he keeps the place locked.\n",
      "Sandra: Prolly building a portable teleporter or something. ;-)\n",
      "Gabby: Or a time machine... LOL\n",
      "Sandra: Or a new greatly improved Rabbit :-P\n",
      "Gabby: I wish... Lmfao!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Sandra and Jim are moving into the new house. Sandra's husband Jim is grumpy and keeps the garage locked. Sandra's husband is building a portable teleporter or a time machine.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Rachel: HAPPY NEW YEAAAAAAR!! ヽ(^o^)丿\n",
      "Rachel: from me & Tom :)\n",
      "Jack: Thanks!!\n",
      "Miranda: same to you\n",
      "Miranda: :)\n",
      "Rachel: May you get exactly what you are expecting in the coming year!!!\n",
      "Miranda: :*\n",
      "Jack: How's your party?\n",
      "Rachel: %) %) %) \n",
      "Jack: Hahahah\n",
      "Jack: I see\n",
      "Rachel: the best party eveeeeeeeer\n",
      "Rachel: wish you were here!!!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "Rachel wishes a happy new year from her and Tom to Miranda and Jack. Rachel is having the best time but misses Miranda and Jack.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Rachel and Tom have a party.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Freddie: Just finished House of Cards, any recommendations?\n",
      "John: Versailles! I know it's different, but if you're into politics and scheming that's your show.\n",
      "Olivia: I agree, I watched it all, highly recommend.\n",
      "Freddie: Is it on Netflix? Can't find it.\n",
      "Olivia: No, only Canal+ :(\n",
      "May: How to Get Away With Murder is also great, it's also on Netflix. I'm currently waiting for the new season as they upload with a year long delay.\n",
      "Freddie: How many seasons are there now?\n",
      "May: Five, six coming in 2019.\n",
      "John: Might look into it as well. Have you seen Black Mirror?\n",
      "Freddie: Heard about it, but haven't started yet. Is it any good?\n",
      "Olivia: Hard to tell, couldn't finish the first episode with the pig :x\n",
      "Freddie: Pig? :D\n",
      "May: Well, yeah, there's that one really twisted episode, the first one. You can always skip it as every episode tells a different story. All of them are pretty twisted, but the first one was the worst for me as well.\n",
      "John: It's hard to tell guys if you are encouraging or discouraging him to watch it :D\n",
      "May: Just being honest, John ;) But I agree HTGAWM is a safer option, however weird it may sound. \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "John and May recommended Freddie to watch Versailles and How to Get Away With Murder.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "John recommends Versailles and How to Get Away With Murder. Olivia recommends How to Get Away With Murder and Black Mirror.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's perform the same with samsum\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset_samsum['train'][index]['dialogue']\n",
    "    summary = dataset_samsum['train'][index]['summary']\n",
    "\n",
    "    \n",
    "    # Here in this prompt we are not providing any example\n",
    "    # Rather we are just instructing to summarize the input which is a conversation in our case\n",
    "    prompt = f\"\"\"\n",
    "                Summarize the following conversation.\n",
    "                {dialogue}\n",
    "                Summary:\n",
    "            \"\"\"\n",
    "    \n",
    "\n",
    "    # We pass prompt in place of dialogue in the tokenizer.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Model prediction with tokenized data\n",
    "    model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    "    # Detokenization\n",
    "    outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    print('-'*120)\n",
    "    print('Example Number:   ', i + 1)\n",
    "    print('-'*120)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print('-'*120)\n",
    "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
    "    print('-'*120)  \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{outputs}\\n')\n",
    "    print('-'*120)  \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace03d0b",
   "metadata": {},
   "source": [
    "Despite using zero-shot prompting, the model still struggles to capture the subtle nuances of the conversations. While it can generate coherent responses based on the provided instructions, it often misses context-specific details, tone, and underlying intentions that are crucial for understanding complex dialogues. Zero-shot prompting relies on the model's general training, but without fine-tuning or task-specific examples, the model may fail to fully grasp intricate aspects of the conversation, such as sarcasm, implied meanings, or emotional undertones. This limitation arises because the model isn't explicitly trained on these finer conversational cues. To address this, prompt engineering can help refine instructions, but there are still inherent challenges when dealing with nuanced language, especially in highly specialized or emotional contexts. <br/>\n",
    "Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks, you can check it here - https://github.com/google-research/FLAN/tree/main/flan/v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dd2ab8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Person1 has a cable out for the past week.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The average grade of all the courses is above 85.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The new cleaning unit is a clever design.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset_dialogsum['train'][index]['dialogue']\n",
    "    summary = dataset_dialogsum['train'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\" \n",
    "                Dialogue:\n",
    "                {dialogue}\n",
    "                \n",
    "                What was going on?\n",
    "             \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    " \n",
    "    outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('-'*120)\n",
    "    print('Example Number:   ', i + 1)\n",
    "    print('-'*120)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print('-'*120)\n",
    "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
    "    print('-'*120)  \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{outputs}\\n')\n",
    "    print('-'*120)  \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec474924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: I have a problem with my cable.\n",
      "#Person2#: What about it?\n",
      "#Person1#: My cable has been out for the past week or so.\n",
      "#Person2#: The cable is down right now. I am very sorry.\n",
      "#Person1#: When will it be working again?\n",
      "#Person2#: It should be back on in the next couple of days.\n",
      "#Person1#: Do I still have to pay for the cable?\n",
      "#Person2#: We're going to give you a credit while the cable is down.\n",
      "#Person1#: So, I don't have to pay for it?\n",
      "#Person2#: No, not until your cable comes back on.\n",
      "#Person1#: Okay, thanks for everything.\n",
      "#Person2#: You're welcome, and I apologize for the inconvenience.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# has a problem with the cable. #Person2# promises it should work again and #Person1# doesn't have to pay while it's down.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The cable is down and the person has to pay for it.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The average grade of all the courses is above 85.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example Number:    3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "#Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The new cleaning unit is a clever design.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset_dialogsum['train'][index]['dialogue']\n",
    "    summary = dataset_dialogsum['train'][index]['summary']\n",
    "        \n",
    "    # Using another slightly different prompt\n",
    "    prompt = f\"\"\"\n",
    "                Dialogue:\n",
    "                {dialogue}\n",
    "                \n",
    "                What was going on? Summary: ?\n",
    "              \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    " \n",
    "    outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('-'*120)\n",
    "    print('Example Number:   ', i + 1)\n",
    "    print('-'*120)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print('-'*120)\n",
    "    print(f'HUMAN SUMMARY:\\n{summary}')\n",
    "    print('-'*120)  \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{outputs}\\n')\n",
    "    print('-'*120)  \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6aff9f",
   "metadata": {},
   "source": [
    "Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what we will try to solve with the few shot inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13740f2",
   "metadata": {},
   "source": [
    "#### Summarize Dialogue with One Shot and Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68de17e",
   "metadata": {},
   "source": [
    "**One-shot** and **few-shot inference** are techniques used to improve the performance of large language models (LLMs) by providing them with one or more examples of prompt-response pairs that are representative of the task you want the model to perform. These examples are presented before the actual task prompt, helping the model understand the context and expected output style. This process is known as **in-context learning**.\n",
    "\n",
    "In **one-shot inference**, a single example is provided, while in **few-shot inference**, multiple examples are given. These examples serve as demonstrations of the task, guiding the model on how to generate responses that align with the task's requirements. By learning from these examples, the model adjusts its responses to match the pattern seen in the prompt-response pairs.\n",
    "\n",
    "In-context learning leverages the model's ability to understand patterns in the examples and apply that understanding to new, unseen tasks. It doesn’t require retraining the model but instead relies on the model’s inherent ability to generalize from the provided examples. This makes one-shot and few-shot prompting highly effective for tasks where fine-tuning the model is impractical or resource-intensive.\n",
    "\n",
    "However, the quality of the task-specific response depends heavily on the clarity and relevance of the examples provided. With fewer examples, the model may struggle to fully understand the task's nuances, whereas providing more examples increases the chances of achieving high-quality, contextually accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a63802a",
   "metadata": {},
   "source": [
    "<b/> Let's perform One Shot Inference first </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6c40824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    \n",
    "    for index in example_indices:\n",
    "        dialogue = dataset_samsum['train'][index]['dialogue']\n",
    "        summary = dataset_samsum['train'][index]['summary']\n",
    "        \n",
    "        # The first part of the prompt is providing input and the human summary as an example. There is only one example.\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        # Look into the documentation and use the prompt templates as it is\n",
    "        prompt += f\"\"\"\n",
    "    Dialogue:\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    What was going on?\n",
    "    {summary}\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "    \n",
    "    # The second part of the prompt is only providing the input dialogues without summary.\n",
    "    # The model has to generate the summary\n",
    "    dialogue = dataset_samsum['train'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "    Dialogue:\n",
    "\n",
    "    {dialogue}\n",
    "    \n",
    "    What was going on?\n",
    "            \"\"\"\n",
    "    \n",
    "    # Returning the full prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e42d0787",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Dialogue:\n",
      "\n",
      "    Gabby: How is you? Settling into the new house OK?\r\n",
      "Sandra: Good. The kids and the rest of the menagerie are doing fine. The dogs absolutely love the new garden. Plenty of room to dig and run around.\r\n",
      "Gabby: What about the hubby?\r\n",
      "Sandra: Well, apart from being his usual grumpy self I guess he's doing OK.\r\n",
      "Gabby: :-D yeah sounds about right for Jim.\r\n",
      "Sandra: He's a man of few words. No surprises there. Give him a backyard shed and that's the last you'll see of him for months.\r\n",
      "Gabby: LOL that describes most men I know.\r\n",
      "Sandra: Ain't that the truth! \r\n",
      "Gabby: Sure is. :-) My one might as well move into the garage. Always tinkering and building something in there.\r\n",
      "Sandra: Ever wondered what he's doing in there?\r\n",
      "Gabby: All the time. But he keeps the place locked.\r\n",
      "Sandra: Prolly building a portable teleporter or something. ;-)\r\n",
      "Gabby: Or a time machine... LOL\r\n",
      "Sandra: Or a new greatly improved Rabbit :-P\r\n",
      "Gabby: I wish... Lmfao!\n",
      "\n",
      "    What was going on?\n",
      "    Sandra is setting into the new house; her family is happy with it. Then Sandra and Gabby discuss the nature of their men and laugh about their habit of spending time in the garage or a shed.\n",
      "\n",
      "\n",
      "                \n",
      "    Dialogue:\n",
      "\n",
      "    Richard: I have a feeling that my girl is cheating on me...\r\n",
      "Matt: Well...\r\n",
      "Matt: Don't know what to reply\r\n",
      "Matt: I'm sorry man..\r\n",
      "Matt: But are you sure?\r\n",
      "Richard: I'm not. But I have my reasons to believe so.\r\n",
      "Matt: I once made a mistake.\r\n",
      "Matt: I accussed my girl of cheating on me. But it turned out she conspired with my friends to throw me a suprprise birthday party.\r\n",
      "Matt: She was furious when I confronted her.\r\n",
      "Richard: Wow. You've never told me that, and I was partially resposible for it...\r\n",
      "Matt: Nevermind. Everything is fine now.\r\n",
      "Richard: In my case however it's not about conspiring. It's the distance. She created so much distance between us that I have a feeling there is someone else she likes to be with.\r\n",
      "Matt: I'm sorry. \r\n",
      "Matt: She doesn't seem to be treacherous viper so the best way is just to talk to her.\r\n",
      "Matt: Maybe that would help\r\n",
      "Richard: I will try. Thanks mate.\r\n",
      "Matt: No problem. I'm with you man.\n",
      "    \n",
      "    What was going on?\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "example_indices = [100]\n",
    "example_index_to_summarize = 222\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices, example_index_to_summarize)\n",
    "\n",
    "# Let's see how our prompt looks like\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faeef473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "Richard suspects his girlfriend is cheating on him because of her emotional distance. Matt has once accused his girlfriend of cheating, when in reality she was throwing him a surprise birthday party. Richard will talk to his girlfriend as Matt advises.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "Richard has a feeling that his girl is cheating on him. Matt accuses his girl of cheating on him. Richard is not sure if he is cheating on her. Matt will try to talk to her.\n"
     ]
    }
   ],
   "source": [
    "# Picking up the human written summary to match with the model generated summary\n",
    "summary = dataset_samsum['train'][example_index_to_summarize]['summary']\n",
    "\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "\n",
    "model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    "outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('-'*120)\n",
    "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('-'*120)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362563f",
   "metadata": {},
   "source": [
    "<b> Now let's proceed with Few Shot Inference </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07c7661c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Dialogue:\n",
      "\n",
      "    Joe: R U watching 'The Millionaire'?\r\n",
      "Tim: Sure!\r\n",
      "Jack: Me too!\r\n",
      "Joe: Oooops. the commercial block is finishing.\r\n",
      "Joe: Talk to you later!\n",
      "\n",
      "    What was going on?\n",
      "    Joe, Tim and Jack are watching 'The Millionaire'.\n",
      "\n",
      "\n",
      "                \n",
      "    Dialogue:\n",
      "\n",
      "    Richard: I have a feeling that my girl is cheating on me...\r\n",
      "Matt: Well...\r\n",
      "Matt: Don't know what to reply\r\n",
      "Matt: I'm sorry man..\r\n",
      "Matt: But are you sure?\r\n",
      "Richard: I'm not. But I have my reasons to believe so.\r\n",
      "Matt: I once made a mistake.\r\n",
      "Matt: I accussed my girl of cheating on me. But it turned out she conspired with my friends to throw me a suprprise birthday party.\r\n",
      "Matt: She was furious when I confronted her.\r\n",
      "Richard: Wow. You've never told me that, and I was partially resposible for it...\r\n",
      "Matt: Nevermind. Everything is fine now.\r\n",
      "Richard: In my case however it's not about conspiring. It's the distance. She created so much distance between us that I have a feeling there is someone else she likes to be with.\r\n",
      "Matt: I'm sorry. \r\n",
      "Matt: She doesn't seem to be treacherous viper so the best way is just to talk to her.\r\n",
      "Matt: Maybe that would help\r\n",
      "Richard: I will try. Thanks mate.\r\n",
      "Matt: No problem. I'm with you man.\n",
      "\n",
      "    What was going on?\n",
      "    Richard suspects his girlfriend is cheating on him because of her emotional distance. Matt has once accused his girlfriend of cheating, when in reality she was throwing him a surprise birthday party. Richard will talk to his girlfriend as Matt advises.\n",
      "\n",
      "\n",
      "                \n",
      "    Dialogue:\n",
      "\n",
      "    Miranda: Hi guys, here are the hall menus for this term :)\n",
      "Miranda: <link>\n",
      "Michalis: Thanks Miranda!\n",
      "Linda: Brilliant! \n",
      "\n",
      "    What was going on?\n",
      "    Miranda sends Michalis and Linda a link with the hall menus for this term. \n",
      "\n",
      "\n",
      "                \n",
      "    Dialogue:\n",
      "\n",
      "    Kelly: They called and Matt is leaving for China on Tuesday.\r\n",
      "Jean: Oh no... Already?\r\n",
      "Kelly: Yep. I mean we were expecting it but still I'm a bit bumped\r\n",
      "Jean: I can imagine\r\n",
      "Kelly: There's also the wedding that we were supposed to do together on Saturday\r\n",
      "Jean: Right, Mary's?\r\n",
      "Kelly: Yes. Wanna go as my plus one?\r\n",
      "Jean: I'd have to check with Tim but I don't think we have anything planned :D \r\n",
      "Kelly: Perfect, you're going with me then\r\n",
      "Jean: Yeeey, I was just thinking how it's been quite some time since I went to a wedding\r\n",
      "Kelly: :)\r\n",
      "Jean: :)\n",
      "\n",
      "    What was going on?\n",
      "    Matt is leaving for China on Tuesday. He was supposed to go to Mary's wedding with Kelly. She asks Jean to go with her, and he agrees. The wedding will take place on Saturday. \n",
      "\n",
      "\n",
      "                \n",
      "    Dialogue:\n",
      "\n",
      "    Martha: what will you wear?\r\n",
      "Penny: when?\r\n",
      "Martha: on this company dinner\r\n",
      "Penny: oh, some dress I guess\r\n",
      "Martha: what colour?\r\n",
      "Penny: black?\r\n",
      "Martha: good, that suits u\n",
      "    \n",
      "    What was going on?\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# We will pick more examples in this case\n",
    "\n",
    "example_indices = [111, 222, 333, 444]\n",
    "example_index_to_summarize = 555\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f58567a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "Penny will wear some black dress for the company dinner.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "Penny will wear a black dress on the company dinner.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset_samsum['train'][example_index_to_summarize]['summary']\n",
    "\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "\n",
    "model_generate = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    \n",
    "outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('-'*120)\n",
    "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('-'*120)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0b0f8",
   "metadata": {},
   "source": [
    "In this case, few-shot inference did offer a noticeable improvement over one-shot inference, as providing multiple examples helped the model better understand the desired output format and task. However, increasing the number of examples beyond five or six does not tend to provide significant additional benefits. After this point, the model’s ability to improve its performance plateaus, likely because it has already learned the relevant pattern from the initial examples. \n",
    "\n",
    "It's also crucial to ensure that the number of tokens in the input prompt does not exceed the model's input-context length. In our scenario, the model has a context length limit of 512 tokens. If the total token count surpasses this limit, the excess tokens will be truncated or ignored, potentially leading to incomplete or less accurate outputs. Therefore, it’s important to balance the number of examples provided with the model's token capacity.\n",
    "\n",
    "Despite these limitations, feeding at least one full example (one-shot) into the model can still be highly effective. Even with a single example, the model gains more context and is able to produce a response that is more relevant and coherent. This additional information significantly improves the quality of tasks like summarization, where having a clear structure or format helps the model understand the expectations and generate better summaries. In short, while too many examples can be counterproductive, the right number of examples (especially one or a few) can greatly enhance the model's performance by setting a clear context for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a9da4",
   "metadata": {},
   "source": [
    "### Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1921f",
   "metadata": {},
   "source": [
    "You can customize the output of a large language model (LLM) by adjusting various configuration parameters in the `generate()` method. So far, the primary parameter you’ve been using is `max_new_tokens=100`, which limits the number of tokens the model generates. However, the `generate()` method offers a wide range of other parameters that can influence the output, such as `num_beams`, `temperature`, `top_p`, and `repetition_penalty`, each of which affects how the model generates text.\n",
    "\n",
    "- **`num_beams`**: Controls the number of beams used in beam search. A higher number of beams typically improves the quality of the generated text by exploring more possibilities but increases computation.\n",
    "- **`temperature`**: Modifies the randomness of predictions. A lower temperature (e.g., 0.1) makes the model’s output more deterministic, while a higher temperature (e.g., 1.0) introduces more creativity and variety.\n",
    "- **`top_p`**: Implements nucleus sampling, where the model selects from the smallest set of most probable tokens that have a cumulative probability greater than or equal to `p`. This can lead to more focused or diverse generation.\n",
    "- **`repetition_penalty`**: Discourages the model from repeating the same phrases or tokens by applying a penalty to previously generated tokens. repetition_penalty = 1.0 (No repetition penalty, default behavior)\n",
    "repetition_penalty = 1.5 (Moderate repetition penalty, discourages repetition)\n",
    "repetition_penalty = 2.0 (Strong repetition penalty, significantly reduces repetition)\n",
    "\n",
    "Managing these parameters individually can become cumbersome, especially when you need to experiment with different combinations. A more efficient way to organize and manage these settings is by using the **`GenerationConfig`** class. This class allows you to group related parameters into a single object, making it easier to modify and experiment with different configurations. By encapsulating your generation settings in one place, you can streamline the process and ensure consistency when generating multiple outputs.\n",
    "\n",
    "\n",
    "\n",
    "A full list of available parameters can be found in the Hugging Face Generation documentation - https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4690e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"do_sample\": true,\n",
       "  \"max_new_tokens\": 100,\n",
       "  \"repetition_penalty\": 1.5,\n",
       "  \"temperature\": 0.1\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=100, do_sample=True, temperature=0.1, repetition_penalty=1.5)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "684f2360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "It's snowing in Satle in October.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "It's snowing in Satle.\n"
     ]
    }
   ],
   "source": [
    "# Using the few shot prompt example but with the configured model now\n",
    "\n",
    "# Let's try giving many examples\n",
    "example_indices = [111, 222, 333, 444, 555, 666, 777, 888]\n",
    "example_index_to_summarize = 1111\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices, example_index_to_summarize)\n",
    "\n",
    "\n",
    "summary = dataset_samsum['train'][example_index_to_summarize]['summary']\n",
    "\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "\n",
    "model_generate = model.generate(inputs['input_ids'], generation_config=generation_config,)\n",
    "    \n",
    "outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('-'*120)\n",
    "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('-'*120)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{outputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8507d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "HUMAN SUMMARY:\n",
      "It's snowing in Satle in October.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "It's snowing in Stockholm.\n"
     ]
    }
   ],
   "source": [
    "# Using the few shot prompt example but with the configured model now\n",
    "\n",
    "example_indices = [111, 222, 333, 444]\n",
    "example_index_to_summarize = 1111\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices, example_index_to_summarize)\n",
    "\n",
    "\n",
    "summary = dataset_samsum['train'][example_index_to_summarize]['summary']\n",
    "\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "\n",
    "model_generate = model.generate(inputs['input_ids'], generation_config=generation_config,)\n",
    "    \n",
    "outputs = tokenizer.decode(model_generate[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('-'*120)\n",
    "print(f'HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print('-'*120)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb219cd",
   "metadata": {},
   "source": [
    "I have tried the one shot and few shot prompting using samsum data. Use the dialogsum data and see what you get! Also, try changing the parameters to find out the differences in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ec1a1",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08950ce2",
   "metadata": {},
   "source": [
    "As demonstrated, prompt engineering can significantly enhance the performance of a model for specific use cases, but it does come with certain limitations. This is where **fine-tuning** becomes essential. Fine-tuning allows a model to adapt more specifically to a particular task or dataset, overcoming the general limitations of prompt engineering. For example, if we were using larger models like GPT-3 or LLaMA, which contain billions of parameters, the output quality and accuracy would likely be much higher even with the same process we used with FLAN-T5. These models, due to their massive scale and more sophisticated pre-training, could better handle the task at hand without requiring fine-tuning, especially when the dataset consists of general language rather than highly specialized, domain-specific terms.\n",
    "\n",
    "In such cases, fine-tuning might not be necessary because these larger models are already equipped to handle a wide variety of tasks. The ability to generalize across different types of input is a major advantage of these models. Additionally, there are a variety of **prompt template types** available for models like GPT-3, LLaMA, or others, which provide structured ways to input queries and guide the model to produce more relevant and accurate responses. These templates can range from simple instructions to more complex role-based or chain-of-thought prompting techniques, all of which can be used to enhance the performance of the model without the need for extensive fine-tuning, especially when dealing with less domain-specific datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663862a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
